---
title: "Simulations"
author: "Aaron R. Williams and Jen Andre"
date: now
format: 
  html:
    embed-resources: true
    toc: true
bibliography: references.bib
---

# Setup

## Notation

Let $N$ be the number of observations in the microdata and $z$ be the number of variables in the microdata.

Let $D$ be the vector of cells in a histogram from the $N x z$ microdata including cells that are empty but possible in the data. In other words, $D$ is the Cartesian product of categories in the original $z$ variables in the microdata. Let $d$ be the number of cells in the histogram such that $D = \{1, 2, 3, ..., d\}$.

# Code Workflow

1.  Prep data
2.  Add $D_i$
3.  Add opt-in probability
4.  Add opt-in decision
5.  Calculate histogram with and without noise
6.  Evaluate

## 1. Prep Data

Preparing the data should be handled by functions like `prep001()`. The result should be a tidy microdata set with $N$ rows and $z$ columns. Unused variables should be dropped and there should be no missing values.

## 2. Add $D_i$

Next, we add an ID to map the rows of the microdata to the $d$ cells in the histogram. We use a function called `add_D_i()` to add a variable `D_i`. `D_i` is a generated ID that matches each set of attributes in the microdata to a cell in the histogram.

## 3. Add Opt-In Probability

Next, we add the opt-in probability (`prob_opt_in`) for each row to the microdata. If there is no opt-in option, then this probability should always be `1`. The output of this step should be a tidy microdata set with $N$ rows and $z + 2$ columns.

We will test scenarios with no opt-in, an opt-in with uniform probability of opting in, and an opt-in with differential probabilities of opting in.

## 4. Opt-In Decision

Next, we add the opt-in decision (`opt_in`) by comparing random draws from a standard uniform distribution with the opt-in probability from step 3. The new variable should be logical. The result of this step should be a tidy data frame with $N$ rows and $z + 3$ columns.

## 5. Calculate the Histogram(s)

We will compare centralized and local methods of differential privacy with different hyperparameters ($\epsilon$, opt-in rate, etc.). Each method will have a function starting with `hist_` that takes the microdata and any hyperparameters.

The result will be a tidy data frame with $d$ rows and $z + 4$ columns. The $z$ columns are the original attributes from the microdata, the additional columns `n` and `n_noisy`, and the ID column `D_i`.

## 6. Evaluate

We will look at several metrics. The wrapper function `calc_metrics()` should take the output from step 5 and return a tidy data frame with calculated error metrics.

# Methods

There are three types of protocols for local differential privacy for frequencies:

1.  Direct encoding (GRR)
2.  Unary encoding
3.  Binary local hash

An appealing property of local differential privacy is that it should return the correct number of overall observations within a cell. 

## 1. Direct Encoding (Generalized Random Response)

This is based on the Warner Model [@warner1965]. The main idea is to turn the entire domain into a histogram and then randomly switch observations from one cell to another cell at a rate determined by $\epsilon$. The basic procedure is as follows:

1.  Convert the joint frequency of $z$ variables into $d$ disjoint categories using a Cartesian product
2.  Flip a coin with probability `p`
3.  Report the true value if the coin is heads. Otherwise, report one of the other categories at random with uniform probability.
4.  Sum the true and false responses
5.  Make a correction for the false responses

$p$, the probability of including the true response, connects to $\epsilon$ with the following:

$$p = \frac{e^\epsilon}{e^\epsilon + d - 1}$$

$q$, the probability of selecting one of the untrue responses, connects to $\epsilon$ with the following:

$$q = \frac{1}{e^\epsilon + d - 1}$$

Thus,

$$p + q \cdot (d - 1) = 1$$

Let $\tilde{n_i}$ be the noisy and biased count of observations in cell $D_i$. Then the estimate $\hat{n_i}$ comes from the following:

$$\hat{n_i} = \tilde{n_i} \cdot p + (N - \tilde{n_i}) \cdot q$$

* [Wang Slides](https://www.usenix.org/sites/default/files/conference/protected-files/usenixsecurity17_slides_tianhao_wang.pdf)
* [Programming Differential Privacy](https://programming-dp.com/ch13.html#randomized-response)

## 2. Unary Encoding

https://programming-dp.com/ch13.html#unary-encoding

## 3. Binary Local Hash

Table 4 from Wang

BLENDER

---
format:
  urbn-report-pdf:
    keep-tex: true  
bibliography: bibliography.bib  
---

\pagenumbering{roman}

```{=tex}
\begin{titlepage}
    % Add Policy Center/Intaitive/Toxonmy Term Bar
    % note the textbox exceeds width of document to avoid white space on sides
    \begin{textblock*}{9in}(-0.25in, 0.15in)
        \begin{tcolorbox}[valign = center]
            \begin{center}
                \policycenter{Research Methods and Data Analytics}
            \end{center}
        \end{tcolorbox}
    \end{textblock*}

    % Adding the cover image - code forces the image to be width of full paper (ignoring margins)
    \vspace*{-1.7cm}
    \noindent
    \makebox[\textwidth]{\includegraphics{images/cover.jpg}}
    
    \vspace{0.35in}
    \noindent\textcolor{urban-blue}{\MakeUppercase{\textbf{Research Report}}}
    
    \titlereport{Opt-In Statistical Disclosure Protections}
    
    \vspace{-0.25in}
    
    \reportsubtitle{Empowering Survey Respondents to Improve Data Quality}
    % Multiple column author names - change the "4" to the number of desired columns
    \begin{multicols}{4}
        \authorfont{Aaron R. Williams}
        
        \authorfont{Jennifer Andre}
      
    \end{multicols}
    
    \vspace{-0.75cm}
    
    \datefont{August 2023}

    % Add logo
    \begin{textblock*}{4.5in}[1, 1](5.5in, 10.5in)
        \noindent\includegraphics[width=4.5in]{images/cover-footer.jpg}
    \end{textblock*}
\end{titlepage}
```
\include{frontmatter/about}

\cleardoublepage

```{=tex}
\setcounter{page}{3}
\begin{singlespace}
    \tableofcontents
\end{singlespace}
```
\thispagestyle{empty}

\include{frontmatter/acknowledgements}

\include{frontmatter/executive}

\part{Opt-In Statistical Disclosure Protections}

\section{Introduction}

People generate and share data about themselves every day when they browse the web, interact with government services, and respond to surveys, and they should be empowered to make decisions about how these data are accessed and used. Currently, disclosure protection policies at the US Census Bureau do not allow for such empowerment -- respondents to surveys like the decennial census and the American Community Survey are all subjected to disclosure protections. Data for all respondents are, by default, masked with some form of statistical disclosure control, and those who may wish to see themselves accurately reflected in the data are unable to do so. These data quality distortions may have greater impact for certain groups, such as smaller race and ethnicity groups, relative to others.

In this brief, we explore a new framework for disclosure protections that would require respondents to actively opt in to disclosure protections. Responses for those who opt in would be treated with disclosure protections, while responses for those who forego protections would remain unchanged in statistical product outputs. We present two demonstration studies, the first using an opt-in local differential privacy approach for the decennial census, and the second using an opt-in synthetic data approach for the American Community Survey. In both cases, we seek to explore the impact of varying the rate of opting in to disclosure protections on data quality and the associated privacy consequences. We especially examine the impact for small racial/ethnic groups, including the impact on quality and privacy if some groups opt in at higher rates than others.

We aim to test the feasibility of this potential solution path, contributing to ongoing public discussions and debate about disclosure protections and public data quality involving researchers, public data users, and other stakeholders. This solution would have wide-ranging implications, including operational changes, new outreach strategies, and many complex legal questions about Title 13 and other regulations. The findings we present here provide early evidence on the impact of turning privacy disclosure choices over to participants.

\section{Background}

The primary goal of this report is to present the opt in privacy framework and early evidence on the impact of such a framework on census data quality under two disclosure avoidance methods. The data privacy literature is extensive, and we assume a baseline knowledge of certain key concepts. In this section, we provide a brief overview of a few key concepts, as well as a review of disclosure avoidance at the US Census Bureau.

\subsection{Data Privacy Key Terms}

*Statistical Disclosure Control Methods*

Statistical Disclosure Control (SDC) methods are used to release sensitive or confidential data products while preserving the confidentiality of the data. Traditional SDC methods include suppression, rounding, top- and bottom-coding, synthetic data generation, and more.

*Differential Privacy*

Differential Privacy (DP) is a formal definition of privacy, meaning that DP methods meet certain mathematical properties and guarantees. With DP, it is possible to quantify the amount of privacy loss that occurs with a data release with the "privacy-loss budget", denoted by $\epsilon$. The probability of privacy loss is denoted by $\delta$.

*Utility-Privacy Trade-off*

When applying disclosure avoidance methods to confidential data, there exists a central tension between the usefulness or quality of the resulting "noisy" data and the amount of privacy risk. Before any noise infusion, confidential data have the highest possible utility, but also have high privacy risks. Efforts to improve disclosure protections via SDC methods can reduce these privacy risks, but also may worsen overall data quality and usefulness for intended analyses or other applications. With DP methods, it is possible to "tune" the balance between utility and privacy by changing the value of $\epsilon$, with lower values of $\epsilon$ corresponding to greater noise infusion, implying lower data quality and higher disclosure protection.

\subsection{The US Census Bureau and Disclosure Avoidance}

The US Census Bureau is tasked with providing high quality data about the US and its people. These data are of enormous consequence for the public, serving as the basis for political representation, community funding and planning, and key research. Given these use cases, the accuracy and quality of Census Bureau products is crucial.

Decennial census statistical products are used for congressional apportionment, redistricting, federal funding allocations, planning and decision-making for government and business organizations, and informing many other surveys [@mather2019a]. The American Community Survey (ACS) is used to inform federal policymaking and program delivery, state and local service provision (e.g., roads and schools), and research and analysis by nongovernmental organizations [@unitedstatescensusbureau2017]. In fiscal year 2015, 132 federal programs used Census Bureau data to allocate more than \$675 billion in funds to state and local communities [@hotchkiss2017a]. Decennial census and ACS data are also foundational to racial equity analytics, enabling researchers to answer important research and policy questions [@axelrod2022].

In addition to conducting surveys and releasing high quality public data, the Census Bureau is also obligated to protect the confidentiality of individual respondents reflected in these data products. The Census Bureau's approach to safeguarding the identities of respondents in publicly released data is informed by its interpretation of Section 9 of Title 13 of the US Code, enacted in 1954. This approach has evolved over the years, especially in response to advances in computing technologies and attack methods [@hotz2022a]. In 2018, the Census Bureau announced its intention to "modernize how we protect respondent confidentiality," including the adoption of Differential Privacy (DP) [@abowd2018]. This move was motivated by certain benefits of DP over traditional disclosure limitation, including more robust protections and greater transparency [@abowd2022a].

For the 2020 Decennial Census, the Census Bureau updated their Disclosure Avoidance System (DAS) from traditional swapping algorithms to the TopDown Algorithm (TDA), which refers to a system of DP mechanisms for privacy loss accounting, along with optimization algorithms and post-processing. The TDA satisfies zero-concentrated DP ($\rho$-zCDP), a relaxation of pure DP. As a formally private method, the privacy protections can be quantified, and the Census Bureau does translate the $\rho$-zCDP privacy parameter $\rho$ to the corresponding values of $\epsilon$ and $\delta$ [@bowen2022; @abowd2022a].

In contrast, the Census Bureau has conceded that the "science does not yet exist to comprehensively implement a formally private solution for the ACS" [@daily2022a]. Instead, they are currently exploring the feasibility of a fully synthetic public-use microdata file and accompanying validation server. In both cases, all respondents are subjected to disclosure protections, even those who might otherwise prefer to see their data accurately reflected.

\section{Opt-In Privacy Framework and Implications}

In our demonstrations, we imagine a framework for disclosure protection in which respondents would be asked to actively opt in to statistical disclosure control methods. Those who do not opt-in would simply contribute their true data to statistical products. The choices we make to build this framework may have significant ethical and legal implications, discussed in this section.

\subsection{Opt In Framework Ethical Implications}

The adoption of a framework requiring respondents to opt in to disclosure protections has various ethical considerations. First, a major challenge upon implementation of such a framework would be a significant knowledge gap for respondents. The Census Bureau or any other organization would need to carry out extensive outreach efforts and design plain-language explanations to ensure that respondents understand what they are or are not opting into. It is crucial for respondents to understand the implications of their opt-in choice not only for the privacy of their personal information, but also for the resulting data quality and the downstream impact on their community.

Second, there are ethical considerations in the framing of the opt-in decision and the administration of the question in a survey. For this project, we intentionally chose language of "opting in" to disclosure avoidance protections, treating the decision to forego protections as the default and requiring an additional step for those desiring additional protections. This is in contrast to language of "opting out", which would treat disclosure avoidance protections as the default position. Additionally, we made the simplifying assumption that the primary survey respondent, or householder, makes the opt in decision for all members of the household. This choice may be appropriate if the householder is the parent or legal guardian of a minor in the household, but may not be appropriate for other adults in the household who may disagree with the respondent's opt in decision.

Finally, any application of an opt in framework to disclosure avoidance methods must consider the impact of one respondent choosing to forego protections of the privacy risks of respondents who do opt in. The mathematical guarantees of differential privacy allow for individuals to forego protections without decreasing other respondents' protection, but this is not guaranteed in non-formally private methods like synthetic data generation.

\subsection{Opt In Framework Legal Implications}

In addition to ethical considerations, the implementation of an opt in disclosure avoidance framework would also have significant legal implications. Most notably, the implementation of this type of framework would most likely require a change to Title 13 of the US Code, which could trigger complex legal arguments and potential politicization.

\section{Demonstration 1: Local Differential Privacy for the Decennial Census}

The Census Bureau deployment of DP for the 2020 Decennial Census uses a global approach in which tabulated cells of confidential responses in a series of data tables are infused with noise by a central curator (the Census Bureau). All census respondents are automatically subjected to the DAS, even those who might otherwise wish to see their data reflected accurately, without any noise. Further, the noise that is injected into tabulated data cells is independent of the size of the population in the cell. In effect, there is more relative error added for small groups than for larger ones. This could lead to worse data quality for small groups such as some racial/ethnic groups and, as a result of inaccurate representation in the data, these groups could receive inadequate funding and incorrect research findings.

\subsection{Local Differential Privacy}

To allow for individual-level opt-in, we move from a central DP approach, in which the Census Bureau as a data curator would add noise to all respondents, to a local DP approach, allowing for some respondents to opt in and others to forego disclosure protections. Typically, a local model assumes that a central curator cannot be trusted, and so a respondent adds noise to their data before sending it to the curator. For this use case, we can imagine a slight variation on this approach in which the trusted Census Bureau still receives all data in its confidential form, and then infuses noise only for respondents who opt in.

Many local DP mechanisms are based on the concept of Randomized Response, first proposed by S. L. Warner in 1965 [@warner1965a]. The central idea is that a survey respondent flips a coin, and the result of the coin flips determines if they answer a yes/no question with the true answer or not. The randomization of the coin flip infuses the noise that grants disclosure protections [@near2022].

We use Generalized Random Response (GRR) for our use case, allowing us to move from a binary coin flip to a setting with higher cardinality. With GRR, we turn the entire domain of potential responses into a histogram and randomly switch observations based on a rate determined by the privacy loss budget, $\epsilon$. We then tabulate a resulting histogram of counts and apply an adjustment to account for the randomly perturbed responses [@wang2020a].

\subsection{Data}

For this demonstration, we use person-level records from the 2010 Decennial Census Stateside Public Use Microdata Sample. This sample contains records representing 10 percent of housing units, and the people residing in them, along with 10 percent of people living in group quarters. We restrict our sample to Washington, DC and Iowa because the former has two large racial/ethnic groups, while the latter is more homogeneous. These data contain demographic and household characteristics about respondents, including age, race, ethnicity, and sex.

\subsection{Simulations}

For our simulation approach, we run iterations of a disclosure mechanism to generate noisy histograms of counts for a set of defined attributes. We focus on two disclosure mechanisms to compare a standard global approach to a local opt-in approach. The first is a Laplace sanitizer, a global method in which cells are infused with noise from a Laplace distribution. The Laplace distribution is centered at zero and the variability is the ratio of the privacy loss budget, $\epsilon$, over the $l_1$-global sensitivity of the statistic [@dwork2006c; @williams2023a]. The second is the previously described GRR method, a local method in which individuals who opt in to disclosure protections report a true response with probability $p = \frac{e^\epsilon}{e^\epsilon + d - 1}$, where $d$ is the overall cardinality of possible response values. Otherwise, the record is randomly replaced with another combination of fields. Though the TDA deployed by the Census Bureau is a more complicated series of algorithms and processing steps than the simple Laplace mechanism presented here, this simplification allows us to compare more easily with a simple local approach and focus specifically on the impact of the opt in framework on data quality.

The specifications for our simulations are as follows. For each combination of specifications, we run 100 iterations of each disclosure mechanism.

-   Scenarios: the set of grouping attributes for the resulting histogram frequencies
    -   Scenario 1 (cardinality = 2)
        -   Hispanicity: Hispanic or Latino, Not Hispanic or Latino
    -   Scenario 2 (cardinality = 24)
        -   Age bucket: Child (0-17), Adult (18-64), Senior (65+)

        -   Race/Ethnicity: White alone, Black or African American alone, Other alone, or Hispanic or Latino (any race)

        -   Sex: Male, Female
-   Privacy loss budget, $\epsilon$
    -   1
    -   5
    -   10
    -   20
-   Opt-in rate: the probability that respondents opt in to disclosure protections
    -   0.01

    -   0.1

    -   0.5

    -   0.9

    -   1

\subsection{Evaluation}

We evaluate the results of these simulations using bias and accuracy metrics, comparing the noisy histograms generated under each privacy approach to each other and to the true values. We use mean percent error to evaluate bias, or the tendency for noisy estimates to systematically move in one direction relative to the true values. We use absolute mean percent error to evaluate accuracy, or the closeness of noisy estimates to the true values.

\subsection{Results and Discussion}

**Local DP Methods Result in Overall Lower Accuracy than Global Methods**

For Scenario 1, we focus primarily on results allowing us to compare the performance of the local GRR method to the central Laplace method. With a cardinality of just 2 (Hispanic or Latino, Not Hispanic or Latino), this scenario allows for the most similar comparison with the global method (in which epsilon is allocated to just one statistic).

Figure \ref{fig:methods-bias} shows the distribution of bias metrics from our simulations at the defined levels of $\epsilon$ and opt in rate. Both the local and central methods are unbiased, with the distribution of mean percent error values centered around zero.

```{=tex}
\begin{figure}[!htb]
    \centering
    \caption{Local and Central DP Approaches are Similarly Unbiased}
    \includegraphics[width=4in]{../figures/methods_bias.png}
    \label{fig:methods-bias}
\end{figure}
```
Figure \ref{fig:methods-accuracy} shows the distribution of accuracy metrics from our simulations at the defined levels of epsilon and opt in rate.

```{=tex}
\begin{figure}[!htb]
    \centering
    \caption{Local Method Outperforms Central Method Only with Very High Privacy Loss Budget}
    \includegraphics[width=4in]{../figures/methods_accuracy.png}
    \label{fig:methods-accuracy}
\end{figure}
```
Figure \ref{fig:methods-accuracy} demonstrates two key takeaways about the accuracy of these methods. First, the opt-in framework approach does improve the overall accuracy of the local GRR method. As we decrease the level of opt in, the width of the mean percent error distribution shrinks and moves closer to zero. However, the second key takeaway is that the central method significantly outperforms the local method in terms of accuracy at nearly every tested level of $\epsilon$ and opt in rate, even with very small opt in rates. The local method only outperforms the central method with a very high privacy loss budget of $\epsilon$ = 20, and the errors for both methods are very small for that level of privacy loss anyway.

While the opt-in local approach does improve the accuracy of estimates with lower levels of opt in, this improvement alone is unfortunately not enough to justify a switch from a central model to a local model. Existing local DP methods cannot offer the same level of accuracy as central methods, especially for datasets with even higher cardinality. However, the potential to improve data quality results with a local method and opt-in framework motivates greater focus on developing local DP methods in the future.

**Opt-in Privacy Offers the Potential to Improve Data Quality for Small Groups**

Although existing local DP methods may be disappointing for overall accuracy, an opt-in local DP framework still offers the potential to improve data quality for small groups. Data quality may be especially improved for groups that opt-in at relatively lower rates than others. For Scenario 2, we focus on results allowing us to compare differences in data quality by racial/ethnic group.

Figures \ref{fig:groups_dc} and \ref{fig:groups_ia} show the distribution of accuracy results for the specified opt in rates for each racial/ethnic group (using $\epsilon$ = 1), separately by state.

```{=tex}
\begin{figure}[!htb]
    \centering
    \caption{Accuracy By Racial/Ethnic Group, Washington, DC}
    \includegraphics[width=4in]{../figures/groups_dc.png}
    \label{fig:groups_dc}
\end{figure}
```
```{=tex}
\begin{figure}[!htb]
    \centering
    \caption{Accuracy By Racial/Ethnic Group, Iowa}
    \includegraphics[width=4in]{../figures/groups_ia.png}
    \label{fig:groups_ia}
\end{figure}
```
According to the 2010 Census Redistricting Data (Public Law 94-171) Summary File, the population of Washington DC was 38% white, non-Hispanic and 51% Black, non-Hispanic, and the population of Iowa was 91% white, non-Hispanic. For both states, mean percent error is smallest for these relatively large groups, reflecting the larger sample sizes. Error tends to be relatively larger, and with larger spreads, for the smaller groups in both places.

The opt-in framework offers a solution path to improve data accuracy for these smaller racial/ethnic groups. For example, the median absolute percent error for the Hispanic group is roughly 3.5% in Washington, DC and 6% in Iowa when there is 100% opt in, or when all respondents are subjected to disclosure protections. These error values shrink to about 1.5% and 2%, respectively, with a group opt-in rate of 10%. Given the properties of formal privacy, the privacy protections afforded to those who opt-in are unaffected by those who choose to forego protections.

With an opt-in disclosure framework, the US Census Bureau and community groups could engage in outreach efforts, especially to smaller groups, to help respondents understand the implications of foregoing disclosure protections, both for their privacy but also for the wide-ranging impacts of improving their data quality. This type of outreach could result in better data quality for these groups, with positive downstream impacts on representation and funding allocations to communities.

All in all, existing local DP methods generate protected data of overall lower accuracy than data generated by central models. However, this demonstration shows the potential of local DP to improve data accuracy for small groups, while still protecting privacy, with an opt-in DP framework. This use case motivates further development of local DP methods and opt in experimentation to improve accuracy results.

\section{Demonstration 2: Synthetic Data for the American Community Survey}

The Census Bureau is investigating the creation of a fully synthetic American Community Survey paired with a validation server[^1]. The Census Bureau intended to create formally private data by 2025 but later conceded that the science does not exist yet to comprehensively implement a formally private solution for the ACS[^2]. Next, the Census Bureau intended to release a non-formally private ACS in 2024 but their timeline has been delayed because of legitimate concerns about the impact of synthetic data.[^3]

[^1]: https://www.ipums.org/changes-to-census-bureau-data-products

[^2]: https://www.census.gov/newsroom/blogs/random-samplings/2022/12/disclosure-avoidance-protections-acs.html

[^3]: https://acsdatacommunity.prb.org/discussion-forum/m/2021-acs-conference-files/147/download

Synthetic data could potentially harm the usefulness of the American Community Survey. Groups with fewer observations, include smaller racial and ethnic groups, could see the biggest losses in data quality. An opt-in disclosure protection methodology paired with synthetic data could mitigate the harms of data synthesis and give outreach organizations a tool to improve data quality.

\subsection{Synthetic Data}

Demonstration 1 focused on summary statistics calculated on microdata. We now pivot to a demonstration where the goal is to produce high-quality microdata that can be used for a range of valid analyses. We will pursue this goal with synthetic data generation.

Synthetic data generation is a statistical disclosure control method that replaces confidential microdata with pseudo microdata that can maintain the statistical properties of the confidential data while limiting disclosure risks. @syntheti2022 and @hu2023 offer thorough introductions to synthetic data. We will briefly introduce topics central to this demonstration.

There are two main flavors of synthetic data. With partially synthetic data, some but not all variables are synthesized [@little1993]. With fully synthetic data, all variables are synthesized [@rubin1993]. Fully synthetic data provides stronger disclosure protection.

Partially synthetic data maintains a one-to-one mapping between observations in the synthetic data and observations in the GSDS. This creates identity disclosure risks and increases attribute disclosure risks. It also means it is possible to calculate disclosure metrics based on re-identification [@reiter2009].

Fully synthetic data don't have a one-to-one mapping because every record is fully generated. This minimizes identity disclosure risks attribute disclosure risks, but dramatically reduces approaches for evaluating disclosure risks. We create a fully synthetic version of the ACS for this demonstration.

It is possible to create formally private synthetic data in idealized situations [@bowen2021]. Model-based approaches generally require discretizing categorical variables. Promising approaches generate 1-, 2-, and 3-way marginals and then use graphical models to generate synthetic data [@mckenna2019]. Other approaches use GANs but generally don't work well outside of idealized situations with modest privacy budgets [@tao2021]. Like the Census Bureau, we abandon formal privacy for this demonstration because it is currently infeasible for the ACS. This means we no longer have a provable bound on the worst-case privacy loss.

We use a fully conditional specification (FCS) to generate synthetic data. FCS uses a sequential approach to model the joint distribution of the ACS as a sequence of univariate conditional distributions. We use non-parametric decision trees and regression trees because they are easy to fit and model relatively complex distributions with ease [@reiter2005]. We modify the decision trees and regression trees so that predictions are samples from the final nodes instead of using the means and modes of the nodes. This increases the sample variances in the synthetic data.

We use the tidysynthesis[^4] and syntheval R packages to synthesize data and evaluate synthetic data.

[^4]: "The tidysynthesis R package." Presentation given at rstudio::conf(2022), Washington, DC, July 25 -- 28.

\subsection{Data}

For this demonstration, we use person-level records from the 2021 1-Year American Community Survey (ACS). The ACS is a roughly 1-in-100 sample of households in the US and is the premier source of information for small area estimation. The 2021 ACS Public Use Microdata Sample (PUMS) contains about 3.25 million individuals and 1.44 million households. We implement an opt-in disclosure protection demonstration using synthetic data and the 2021 1-Year American Community Survey. We access all ACS data through IPUMS [@ruggles].

We restrict the data in five important ways to minimize computation and simplify comparisons. First, we restrict our data to observations from Florida, Michigan, and Pennsylvania. We chose these states because they are large states with different types of racial and ethnic diversity. Second, we restrict our data to heads of households ages 18 or older who are not in group quarters. This limits structurally missing values. Furthermore, synthesizing relationships within households is a major technical challenge [@benedetto2020]. Third, we only synthesize a subset of variables.

-   State (categorical)
-   Sex (binary)
-   Age (numeric)
-   Marital status (categorical)
-   Race (categorical)
-   Hispanicity (categorical)
-   Any health insurance coverage (binary)
-   Educational attainment (categorical)
-   Employment status (categorical)
-   Labor force participation (binary)
-   Total family income (numeric)
-   Total personal income (numeric)
-   Wage and salary income (numeric)
-   Welfare income (numeric)
-   Income residual (numeric)

The income residual is personal income minus wages and salary and welfare income. The income residual captures all other sources of income including business income; Social Security income; SSI income; interest, dividend, and rental income; retirement income; and other income.

Fourth, we ignore weights during synthesis and when calculating metrics. Synthesis with weights is an open area of research. Fifth and finally, we randomly partition the data into two halves. The half we synthesize is the gold standard data set (GSDS). The other half is a holdout data set (HDS) for calculating disclosure metrics.

PUMS include statistical disclosure limitation to protect the confidentiality of responses. For example, high incomes are topcoded. When synthesizing data, the Census Bureau has more observations and unaltered variables that could improve the quality of the synthesis but also worsen utility metrics because out metrics do not reflect current SDC techniques.

\subsection{Simulations}

To evaluate the benefits and costs of opt-in disclosure protection we implement a simulation study. We vary two key parameters for each specification. First, we vary the opt-in rate with the values 0.1, 0.5, 0.9, and 1. For example, 90% of observations are unaltered and 10% of observations are synthesized when the opt-in rate is set to 0.1. All observations are synthesized when the opt-in rate is 1. We call the result of this SDC approach candidate data. The opt-in decision for each observation is stochastic. We use a function to set the probability of opt-in and then randomly select the opt-in decision based on that probability. Second, we vary the nature of the opt-in. Under the default settings, all observations have the same opt-in probability within a specification.

It's plausible that different racial and ethnic groups would opt into disclosure protections at different rates. We vary individual opt-in propensities based on race. In addition to equal opt-in probabilities, we implement a white multiplier such that white individuals opt in at half the rate of other individuals (0.5) and twice the rate of other individuals (2). The white multiplier is approximate because the opt-in decisions are stochastic and because the multiplier is imprecise at high levels of opt in.

We need to pick an order to synthesize the variables since we adopt a sequential approach. Typically, more information is preserved for a synthetic variable by moving it from later in the synthesis order to earlier in the synthesis order. We prioritize race and ethnicity. Next, we synthesize the remaining categorical variables. Next, we synthesize age and educational attainment. We treat both as numeric variables. Finally, we synthesize all of the income variables.

We implement a simple synthesis without much customization. We use the rpart implementation of decision trees and regression trees [@therneau2022] using their default values (minsplit = 20, minbucket = 8, and cp = 0.01). These parameters keep the trees from going too deep and overfitting the data.

We run each specification five times to evaluate the simulation-to-simulation variation in metrics.

\subsection{Evaluation}

We evaluate the results of the simulations with general utility metrics, specific utility metrics, and disclosure risk metrics. Descriptions of each metrics are available in the appendix. Complete results are available in the Urban Institute data catalog.

General utility measures the univariate and multivariate distributional similarity between the gold standard data and the synthetic data (e.g., comparing the medians for all numeric variables). Specific utility measures the similarity of results for a specific analysis of the gold standard data and synthetic data (e.g., comparing the coefficients in regression models). Disclosure risk metrics *estimate* the risk of attribute and membership inference attacks.

We calculate the following metrics for all simulations:

**General utility**

-   Absolute error for proportions for all categorical variables
-   Absolute error for proportions for all categorical variables by simplified race/ethnicity and detailed race
-   Absolute proportion error for means for all numeric variables
-   Absolute proportion error for means for all numeric variables by simplified race/ethnicity and detailed race
-   Proportion error in percentiles for numeric variables
-   k-marginal score for all 1-way, 2-way, and 3-way marginals for categorical variables
-   Mean absolute error in pairwise correlation coefficients for numeric variables
-   Discriminator ROC AUC

**Specific utility**

-   Regression confidence interval overlap for a Mincer model

**Disclosure risk**

-   $\ell$-diversity for regression trees summarized for each numeric variable
-   Attribute inference test on welfare income
-   Membership inference test

\subsection{Results and Discussion}

\subsection{Metrics Appendix}

*This is a placeholder section*

**General utility**

-   **Absolute error for proportions for all categorical variables:** For each categorical variable, calculate the proportion of observations with each class for the GSDS and candidate data. Calculate the absolute value of the difference between the GSDS and candidate data.

-   **Absolute error for proportions for all categorical variables by simplified race/ethnicity and detailed race:** For each categorical variable, calculate the proportion of observations with each class for each demographic group for the GSDS and candidate data. Calculate the absolute value of the difference between the GSDS and candidate data.

-   **Absolute proportion error for means for all numeric variables:** For each numeric variable, calculate the mean for the GSDS and candidate data. Calculate the absolute value of the difference between the GSDS and candidate data.

-   **Absolute proportion error for means for all numeric variables by simplified race/ethnicity and detailed race:** For each numeric variable, calculate the mean for the GSDS and candidate data for each demographic group. Calculate the absolute value of the difference between the GSDS and candidate data.

-   **Proportion error in percentiles for numeric variables:** For each numeric variable, calculate the minimum, 10th percentile, 20th percentile, 30th percentile, 40th percentile, 50th percentile, 60th percentile, 70th percentile, 80th percentile, 90th percentile, and maximum. Calculate the proportion difference between the GSDS and candidate data.

-   **k-marginal score for all 1-way, 2-way, and 3-way marginals for categorical variables:** [@raab2021; @sen2023] Select $k$. Let $v$ be the number of categorical variables. Then there are ${v \choose k}$ $k$-marginals. Exhaustively calculate the proportion of observations in each cell for all $k$-way marginals for categorical variables. For each $k$-marginal, calculate the mean absolute error (this is called mean absolute difference between distributions). Finally, make the metric ascending and rescale to max at 1,000 with $(1 - mean(MADD)) \cdot 1000$.

-   **Mean absolute error in pairwise correlation coefficients for numeric variables:** For all numeric variables, calculate a Pearson's linear correlation matrix on the GSDS and candidate data. Difference the lower triangle of the matrix for the GSDS and the matrix for the candidate file. Calculate the mean absolute error.

-   **Discriminator ROC AUC:** Discriminator metrics measure how well a predictive model can distinguish between observations from the GSDS and candidate data. Ideally, the models perform poorly. The p-MSE ratio [@woo2009; @snoke2018] and SPECKS [@bowen2021a] summarize the propensity scores from discriminant models. We take a different approach and look at the ROC AUC for the discriminant models. The three measures are highly correlated, the values for ROC AUC are more familiar to predictive modelers, and the measure doesn't require bootstrapping.

**Specific utility**

-   **Regression confidence interval overlap** measures the overlap of confidence estimated on the gold standard data and synthetic data [@karr2006; @snoke2018]. 1 represents perfect overlap, 0 represents adjacency with no overlap, and negative values represent gaps between the confidence intervals. We regress log wages and salary income on potential experience, potential experience squared, an indicator for white, and sex and calculate the regression confidence interval overlap. This is a simplified version of a "Mincer model" [@card1999].

**Disclosure risk**

-   **$\ell$-diversity for regression trees summarized for each numeric variable:** [@bowen2020] We are concerned that our synthesizer could memorize the confidential data or too closely fit the confidential data. We calculate $\ell$-diversity [@machanavajjhala2007] on the nodes from decision trees and regression trees to measure the heterogeneity of values in the nodes. We calculate the proportion of synthetic values for each variable in the candidate data that come from nodes with low heterogeneity $\ell < 10$.

-   **Attribute inference test on welfare income:** [@elemam2020] We are concerned that an attacker could train predictive models on the synthetic data and make precise inferences about observations in the confidential data. We train a regression tree to predict welfare income on the synthetic data using all other variables as predictors. We assume that an attacker knows all variables but welfare income for the confidential data. We make predictions and calculate the RMSE. We repeat this process using the holdout data instead of the synthetic data to benchmark against the RMSE of models trained when an attacker has access to another random sample from the population instead of the synthetic data.

-   **Membership inference test:** [@zhang2022] We are concerned that an attacker could determine if a confidential observation is in the training data for the candidate file. We construct a data set that is about 10,000 observations from the GSDS and 10,000 observations from the holdout data. For each observation, we measure Gower's distance [@gower1971] to all synthetic observations. Traditionally, analysts pick different thresholds to predict membership and then calculate precision. We normalize the distances so they are in \[0, 1\] and are like probabilities. We then calculate the ROC AUC to measure the tradeoff between true positives and false positives for different thresholds. ROC AUC closer to 0.5 indicates difficulty assessing membership.

\section{Conclusion}

\include{backmatter/appendix}

\include{backmatter/notes}

\include{backmatter/references}

\include{backmatter/author}

\include{backmatter/independence}

```{=tex}
\newpage
\thispagestyle{empty}
```
```{=tex}
\begin{textblock*}{8.5in}[1, 1](8.5in, 11in)
    \noindent\includegraphics[width=\paperwidth,height=\paperheight]{images/back.pdf}
\end{textblock*}
```

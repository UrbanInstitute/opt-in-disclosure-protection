---
format:
  urbn-report-pdf:
    keep-tex: true  
bibliography: bibliography.bib  
---

\pagenumbering{roman}

```{=tex}
\begin{titlepage}
    % Add Policy Center/Intaitive/Toxonmy Term Bar
    % note the textbox exceeds width of document to avoid white space on sides
    \begin{textblock*}{9in}(-0.25in, 0.15in)
        \begin{tcolorbox}[valign = center]
            \begin{center}
                \policycenter{Research Methods and Data Analytics}
            \end{center}
        \end{tcolorbox}
    \end{textblock*}

    % Adding the cover image - code forces the image to be width of full paper (ignoring margins)
    \vspace*{-1.7cm}
    \noindent
    \makebox[\textwidth]{\includegraphics{images/cover.jpg}}
    
    \vspace{0.35in}
    \noindent\textcolor{urban-blue}{\MakeUppercase{\textbf{Research Report}}}
    
    \titlereport{Opt-In Statistical Disclosure Protections}
    
    \vspace{-0.25in}
    
    \reportsubtitle{Empowering Survey Respondents to Improve Data Quality}
    % Multiple column author names - change the "4" to the number of desired columns
    \begin{multicols}{4}
        \authorfont{Aaron R. Williams}
        
        \authorfont{Jennifer Andre}
      
    \end{multicols}
    
    \vspace{-0.75cm}
    
    \datefont{August 2023}

    % Add logo
    \begin{textblock*}{4.5in}[1, 1](5.5in, 10.5in)
        \noindent\includegraphics[width=4.5in]{images/cover-footer.jpg}
    \end{textblock*}
\end{titlepage}
```
\include{frontmatter/about}

\cleardoublepage

```{=tex}
\setcounter{page}{3}
\begin{singlespace}
    \tableofcontents
\end{singlespace}
```
\thispagestyle{empty}

\include{frontmatter/acknowledgements}

\include{frontmatter/executive}

\part{Opt-In Statistical Disclosure Protections}

\section{Introduction}

People generate and share data about themselves every day when they browse the web, purchase goods and services, and respond to surveys, and they should be empowered to make decisions about how these data are accessed and used. Currently, disclosure protection policies at the US Census Bureau do not allow for such empowerment -- respondents to surveys like the decennial census and the American Community Survey are all subjected to disclosure protections. Data for all respondents are, by default, masked with some form of statistical disclosure control, and those who may wish to see themselves accurately reflected in the data are unable to do so. These data quality distortions may have greater impact for certain groups, such as smaller race and ethnicity groups, relative to others.

In this brief, we explore a new framework for disclosure protections that would require respondents to actively opt in to disclosure protections. Responses for those who opt in would be treated with disclosure protections, while responses for those who forego protections would remain unchanged in statistical product outputs. We present two demonstration studies, the first using an opt-in local differential privacy approach for the decennial census, and the second using an opt-in synthetic data approach for the American Community Survey. In both cases, we seek to explore the impact of varying the rate of opting in to disclosure protections on data quality, along with the associated privacy consequences. We especially examine the impact for small racial/ethnic groups, including the impact on quality and privacy if some groups opt in at higher rates than others.

We aim to test the feasibility of this potential solution path, contributing to ongoing public discussions and debate about disclosure protections and public data quality involving researchers, public data users, and other stakeholders. This solution would have wide-ranging implications, including operational changes, new outreach strategies, and many complex legal questions about Title 13 and other regulations. The findings we present here provide early evidence on the impact of turning privacy disclosure choices over to participants.

\section{Background}

\subsection{Data Privacy}

Utility & privacy tradeoff

Formal privacy/DP

Privacy loss budget

Traditional SDC

Synthetic data

\subsection{The US Census Bureau and Disclosure Avoidance}

The US Census Bureau is tasked with providing high quality data about the US and its people. These data are of enormous consequence for the public, serving as the basis for political representation, community funding and planning, and key research. Given these use cases, the accuracy and quality of Census Bureau products is crucial.

Decennial census statistical products are used for congressional apportionment, redistricting, federal funding allocations, planning and decision-making for government and business organizations, and informing many other surveys [@mather2019a]. The American Community Survey (ACS) is used to inform federal agencies for policymaking and program delivery, state and local agencies for services like roads and schools, and nongovernmental organizations for research and analysis [@unitedstatescensusbureau2017]. In fiscal year 2015, 132 federal programs used Census Bureau data to allocate more than \$675 billion in funds to state and local communities [@hotchkiss2017a]. Decennial census and ACS data are also foundational to racial equity analytics, enabling researchers to answer important research and policy questions [@axelrod2022].

In addition to conducting surveys and releasing high quality public data, the Census Bureau is also obligated to protect the confidentiality of individual respondents reflected in these data products. The Census Bureau's approach to safeguarding the identities of respondents in publicly released data is informed by its interpretation of Section 9 of Title 13 of the U.S. Code, enacted in 1954. This approach has evolved over the years, especially in response to advances in computing technologies and attack methods [@hotz2022a]. In 2018, the Census Bureau announced its intention to "modernize how we protect respondent confidentiality," including the adoption of Differential Privacy (DP) [@abowd2018]. This move was motivated by certain benefits of DP over traditional disclosure limitation, including more robust protections and greater transparency [@abowd2022a].

For the 2020 Decennial Census, the Census Bureau updated their Disclosure Avoidance System (DAS) from traditional swapping algorithms to the differentially private TopDown Algorithm (TDA) [@bowen2022]. As a formally private method, the privacy protections can be quantified. In contrast, the Census Bureau has conceded that the "science does not yet exist to comprehensively implement a formally private solution for the ACS" [@daily2022a]. Instead, they are currently exploring the feasibility of a fully synthetic public-use microdata file and accompanying validation server. In both cases, all respondents are subjected to disclosure protections, even those who might otherwise prefer to see their data accurately reflected.

\section{Opt-In Privacy Framework and Implications}

In our demonstrations, we imagine a framework for disclosure protection in which respondents would be asked to actively opt in to statistical disclosure control methods. Those who do not opt-in would simply contribute their true data to statistical products. We make various simplifying assumptions and decisions, described in detail later in this report. 

Such a change in framework would have significant legal ramifications such as X, Y, Z. 

There are also various ethical considerations. For one, an opt-in framework would require significant outreach efforts and plain-language explanations to ensure that respondents understand what they are or are not opting into. Further, careful attention must be paid to the impact of one respondent choosing to forego protections on the privacy risks of respondents who do opt in. 

\section{Demonstration 1: Local Differential Privacy for the Decennial Census}

The Census Bureau deployment of DP for the 2020 Decennial Census uses a global privacy approach in which tabulated cells of confidential responses in a series of data tables are infused with noise by a central curator (the Census Bureau). All census respondents are automatically subjected to the DAS, even those who might otherwise wish to see their data reflected accurately, without any noise. Further, the noise that is injected into tabulated data cells is independent of the size of the population in the cell. In effect, there is more relative error added for small groups than for larger ones. This could lead to worse data quality for small groups such as some racial/ethnic groups and, as a result of inaccurate representation in the data, these groups could receive inadequate funding and incorrect research findings. 

A primary benefit of a formal privacy approach like the DAS is that the overall privacy loss budget can be transparently quantified with the metric $\epsilon$. Further, the mathematical properties of DP allow for some individuals to forego privacy protections without decreasing other respondents' protections. 

\subsection{Local Differential Privacy}

To allow for individual-level opt-in, we move from a central DP approach, in which the Census Bureau as a data curator would add noise to all respondents, to a local DP approach, allowing for some respondents to opt in and others to forego disclosure protections. Typically, a local model assumes that a central curator cannot be trusted, and so a respondent adds noise to their data before sending it to the curator. For this use case, we can imagine a slight variation on this approach in which the trusted Census Bureau still receives all data in its confidential form, and then infuses noise only for respondents who opt in. 

Many local DP mechanisms are based on the concept of Randomized Response, first proposed by S. L. Warner in 1965 [@warner1965a]. The central idea is that a survey respondent flips a coin, and the result of the coin flips determines if they answer a yes/no question with the true answer or not. The randomization of the coin flip infuses the noise that grants disclosure protections [@near2022].

We use Generalized Random Response (GRR) for our use case, allowing us to move from a binary coin flip to a setting with higher cardinality. With GRR, we turn the entire domain of potential responses into a histogram and randomly switch observations based on a rate determined by the privacy loss budget, $\epsilon$. We then tabulate a resulting histogram of counts and apply an adjustment to account for the randomly perturbed responses [@wang2020a].

\subsection{Data}

For this demonstration, we use person-level records from the 2010 Decennial Census Stateside Public Use Microdata Sample. This sample contains records representing 10 percent of housing units, and the people residing in them, along with 10 percent of people living in group quarters. We restrict our sample to Washington, DC and Iowa because the former has two large racial/ethnic groups, while the latter is more homogeneous. These data contain demographic and household characteristics about respondents, including age, race, ethnicity, and sex.

\subsection{Simulations}

For our simulation approach, we run iterations of a disclosure mechanism to generate noisy histograms of counts for a set of defined attributes. We focus on two disclosure mechanisms to compare a standard global approach to a local opt-in approach. The first is a Laplace sanitizer, a global method in which cells are infused with noise from a Laplace distribution. The Laplace distribution is centered at zero and the variability is the ratio of the privacy loss budget, $\epsilon$, over the $l_1$-global sensitivity of the statistic [@dwork2006c][@williams2023a]. The second is the previously described GRR method, a local method in which individuals who opt in to disclosure protections report a true response with probability $p = \frac{e^\epsilon}{e^\epsilon + d - 1}$. Otherwise, the record is randomly replaced with another combination of fields. 

We then evaluate these results using bias and accuracy metrics, comparing the noisy histograms to the true values. We use mean percent error to evaluate bias, or the tendency for noisy estimates to systematically move in one direction relative to the true values. We use absolute mean percent error to evaluate accuracy, or the closeness of noisy estimates to the true values. 

The specifications for our simulations are as follows. For each combination of specifications, we run 100 iterations of each disclosure mechanism.

-   Scenarios: the set of grouping attributes for the resulting histogram frequencies
    -   Scenario 1 (cardinality = 2)
        -   Hispanicity: Hispanic or Latino, Not Hispanic or Latino
    -   Scenario 2 (cardinality = 24)
        -   Age bucket: Child (0-17), Adult (18-64), Senior (65+)

        -   Race/Ethnicity: White alone, Black or African American alone, Other alone, or Hispanic or Latino (any race)

        -   Sex: Male, Female
-   Privacy loss budget, $\epsilon$
    -   1
    -   5
    -   10
    -   20
-   Opt-in rate: the probability that respondents opt in to disclosure protections
    -   0.01

    -   0.1

    -   0.5

    -   0.9

    -   1

\subsection{Results and Discussion}

**Local DP Methods Result in Overall Lower Accuracy than Global Methods**

For Scenario 1, we focus primarily on results allowing us to compare the performance of the local GRR method to the central Laplace method. With a cardinality of just 2 (Hispanic or Latino, Not Hispanic or Latino), this scenario allows for the most similar comparison with the global method (in which epsilon is allocated to just one statistic). 

Figure \ref{fig:methods-bias} shows the distribution of bias metrics from our simulations at the defined levels of $\epsilon$ and opt in rate. Both the local and central methods are unbiased, with the distribution of mean percent error values centered around zero. 

```{=tex}
\begin{figure}[!htb]
    \centering
    \caption{Local and Central DP Approaches are Similarly Unbiased}
    \includegraphics[width=4in]{../figures/methods_bias.png}
    \label{fig:methods-bias}
\end{figure}
```
Figure \ref{fig:methods-accuracy} shows the distribution of accuracy metrics from our simulations at the defined levels of epsilon and opt in rate. 

```{=tex}
\begin{figure}[!htb]
    \centering
    \caption{Local Method Outperforms Central Method Only with Very High Privacy Loss Budget}
    \includegraphics[width=4in]{../figures/methods_accuracy.png}
    \label{fig:methods-accuracy}
\end{figure}
```
Figure \ref{fig:methods-accuracy} demonstrates two key takeaways about the accuracy of these methods. First, the opt-in framework approach does improve the overall accuracy of the local GRR method. As we decrease the level of opt in, the width of the mean percent error distribution shrinks and moves closer to zero. However, the second key takeaway is that the central method significantly outperforms the local method in terms of accuracy at nearly every tested level of $\epsilon$ and opt in rate, even with very small opt in rates. The local method only outperforms the central method with a very high privacy loss budget of $\epsilon$ = 20, and the errors for both methods are very small for that level of privacy loss anyway. 

While the opt-in local approach does improve the accuracy of estimates with lower levels of opt in, this improvement alone is unfortunately not enough to justify a switch from a central model to a local model. Existing local DP methods cannot offer the same level of accuracy as central methods, especially for datasets with even higher cardinality. However, the potential to improve data quality results with a local method and opt-in framework motivates greater focus on developing local DP methods in the future. 

**Opt-in Privacy Offers the Potential to Improve Data Quality for Small Groups**

Although existing local DP methods may be disappointing for overall accuracy, an opt-in local DP framework still offers the potential to improve data quality for small groups. Data quality may be especially improved for groups that opt-in at relatively lower rates than others. For Scenario 2, we focus on results allowing us to compare differences in data quality by racial/ethnic group. 

Figures \ref{fig:groups_dc} and \ref{fig:groups_ia} show the distribution of accuracy results for the specified opt in rates for each racial/ethnic group (using $\epsilon$ = 1), separately by state.

```{=tex}
\begin{figure}[!htb]
    \centering
    \caption{Accuracy By Racial/Ethnic Group, Washington, DC}
    \includegraphics[width=4in]{../figures/groups_dc.png}
    \label{fig:groups_dc}
\end{figure}
```
```{=tex}
\begin{figure}[!htb]
    \centering
    \caption{Accuracy By Racial/Ethnic Group, Iowa}
    \includegraphics[width=4in]{../figures/groups_ia.png}
    \label{fig:groups_ia}
\end{figure}
```
According to the 2010 Census Redistricting Data (Public Law 94-171) Summary File, the population of Washington DC was 38% white, non-Hispanic and 51% Black, non-Hispanic, and the population of Iowa was 91% white, non-Hispanic. For both states, mean percent error is smallest for these relatively large groups, reflecting the larger sample sizes. Error tends to be relatively larger, and with larger spreads, for the smaller groups in both places.

The opt-in framework offers a solution path to improve data accuracy for these smaller racial/ethnic groups. For example, the median absolute percent error for the Hispanic group is roughly 3.5% in Washington, DC and 6% in Iowa when there is 100% opt in, or when all respondents are subjected to disclosure protections. These error values shrink to about 1.5% and 2%, respectively, with a group opt-in rate of 10%. Given the properties of formal privacy, the privacy protections afforded to those who opt-in are unaffected by those who choose to forego protections.

With an opt-in disclosure framework, the US Census Bureau and community groups could engage in outreach efforts, especially to smaller groups, to help respondents understand the implications of foregoing disclosure protections, both for their privacy but also for the wide-ranging impacts of improving their data quality. This type of outreach could result in better data quality for these groups, with positive downstream impacts on representation and funding allocations to communities. 

All in all, existing local DP methods generate protected data of overall lower accuracy than data generated by central models. However, this demonstration shows the potential of local DP to improve data accuracy for small groups, while still protecting privacy, with an opt-in DP framework. This use case motivates further development of local DP methods and opt in experimentation to improve accuracy results. 

\section{Demonstration 2: Synthetic Data for the American Community Survey}

\subsection{Synthetic Data}

\subsection{Data}

\subsection{Simulations}

\subsection{Results and Discussion}

\section{Conclusion}

\include{backmatter/appendix}

\include{backmatter/notes}

\include{backmatter/references}

\include{backmatter/author}

\include{backmatter/independence}

```{=tex}
\newpage
\thispagestyle{empty}
```
```{=tex}
\begin{textblock*}{8.5in}[1, 1](8.5in, 11in)
    \noindent\includegraphics[width=\paperwidth,height=\paperheight]{images/back.pdf}
\end{textblock*}
```
